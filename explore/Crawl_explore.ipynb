{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 pages\n",
      "Processed 200 pages\n",
      "Processed 300 pages\n",
      "Processed 400 pages\n",
      "Processed 500 pages\n",
      "Processed 600 pages\n",
      "Processed 700 pages\n",
      "Processed 800 pages\n",
      "Processed 900 pages\n",
      "Processed 1000 pages\n",
      "Processed 1100 pages\n",
      "Processed 1200 pages\n",
      "Processed 1300 pages\n",
      "Processed 1400 pages\n",
      "Processed 1500 pages\n",
      "Processed 1600 pages\n",
      "Processed 1700 pages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "pages = range(1,1730)\n",
    "menu_page = 'https://lyricvn.com/tag/viet-nam/page/{}/'\n",
    "song_pages = set()\n",
    "for page_idx in pages:\n",
    "    url = menu_page.format(page_idx)\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    for tag in soup.find_all('a'):\n",
    "        if 'lyricvn.com/loi-bai-hat' in tag.get('href'):\n",
    "            song_pages.add(tag.get('href'))\n",
    "    if page_idx % 100 == 0:\n",
    "        print('Processed {} pages'.format(page_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "list_song_pages = list(song_pages)\n",
    "file_paths = []\n",
    "segment_file_paths = []\n",
    "chunk = 0 \n",
    "for url_idx,url in enumerate(list_song_pages):\n",
    "    if url_idx % 100 == 0:\n",
    "        chunk += 1\n",
    "    file_name = url.replace('https://lyricvn.com/','').replace('/','')+'.txt'\n",
    "    file_paths.append(f\"raw/chunk_{chunk}/{file_name}\")\n",
    "    segment_file_paths.append(f\"segment/chunk_{chunk}/segment_{file_name}\")\n",
    "    \n",
    "\n",
    "pd_songs = pd.DataFrame(list_song_pages,columns=['song_url'])\n",
    "pd_songs['file_path'] = file_paths\n",
    "pd_songs['segment_file_path'] = segment_file_paths\n",
    "pd_songs.to_csv('data/song_from_lyricvn_com/songs_meta_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 pages\n",
      "Processed 200 pages\n",
      "Processed 300 pages\n",
      "Processed 400 pages\n",
      "Processed 500 pages\n",
      "Processed 600 pages\n",
      "Processed 700 pages\n",
      "Processed 800 pages\n",
      "Processed 900 pages\n",
      "Processed 1000 pages\n",
      "Processed 1100 pages\n",
      "Processed 1200 pages\n",
      "Processed 1300 pages\n",
      "Processed 1400 pages\n",
      "Processed 1500 pages\n",
      "Processed 1600 pages\n",
      "Processed 1700 pages\n",
      "Processed 1800 pages\n",
      "Processed 1900 pages\n",
      "Processed 2000 pages\n",
      "Processed 2100 pages\n",
      "Processed 2200 pages\n",
      "Processed 2300 pages\n",
      "Processed 2400 pages\n",
      "Processed 2500 pages\n",
      "Processed 2600 pages\n",
      "Processed 2700 pages\n",
      "Processed 2800 pages\n",
      "Processed 2900 pages\n",
      "Processed 3000 pages\n",
      "Processed 3100 pages\n",
      "Processed 3200 pages\n",
      "Processed 3300 pages\n",
      "Processed 3400 pages\n",
      "Processed 3500 pages\n",
      "Processed 3600 pages\n",
      "Processed 3700 pages\n",
      "Processed 3800 pages\n",
      "Processed 3900 pages\n",
      "Processed 4000 pages\n",
      "Processed 4100 pages\n",
      "Processed 4200 pages\n",
      "Processed 4300 pages\n",
      "Processed 4400 pages\n",
      "Processed 4500 pages\n",
      "Processed 4600 pages\n",
      "Processed 4700 pages\n",
      "Processed 4800 pages\n",
      "Processed 4900 pages\n",
      "Processed 5000 pages\n",
      "Processed 5100 pages\n",
      "Processed 5200 pages\n",
      "Processed 5300 pages\n",
      "Processed 5400 pages\n",
      "Processed 5500 pages\n",
      "Processed 5600 pages\n",
      "Processed 5700 pages\n",
      "Processed 5800 pages\n",
      "Processed 5900 pages\n",
      "Processed 6000 pages\n",
      "Processed 6100 pages\n",
      "Processed 6200 pages\n",
      "Processed 6300 pages\n",
      "Processed 6400 pages\n",
      "Processed 6500 pages\n",
      "Processed 6600 pages\n",
      "Processed 6700 pages\n",
      "Processed 6800 pages\n",
      "Processed 6900 pages\n",
      "Processed 7000 pages\n",
      "Processed 7100 pages\n",
      "Processed 7200 pages\n",
      "Processed 7300 pages\n",
      "Processed 7400 pages\n",
      "Processed 7500 pages\n",
      "Processed 7600 pages\n",
      "Processed 7700 pages\n",
      "Processed 7800 pages\n",
      "Processed 7900 pages\n",
      "Processed 8000 pages\n",
      "Processed 8100 pages\n",
      "Processed 8200 pages\n",
      "Processed 8300 pages\n",
      "Processed 8400 pages\n",
      "Processed 8500 pages\n",
      "Processed 8600 pages\n",
      "Processed 8700 pages\n",
      "Processed 8800 pages\n",
      "Processed 8900 pages\n",
      "Processed 9000 pages\n",
      "Processed 9100 pages\n",
      "Processed 9200 pages\n",
      "Processed 9300 pages\n",
      "Processed 9400 pages\n",
      "Processed 9500 pages\n",
      "Processed 9600 pages\n",
      "Processed 9700 pages\n",
      "Processed 9800 pages\n",
      "Processed 9900 pages\n",
      "Processed 10000 pages\n",
      "Processed 10100 pages\n",
      "Processed 10200 pages\n",
      "Processed 10300 pages\n",
      "Processed 10400 pages\n",
      "Processed 10500 pages\n",
      "Processed 10600 pages\n",
      "Processed 10700 pages\n",
      "Processed 10800 pages\n",
      "Processed 10900 pages\n",
      "Processed 11000 pages\n",
      "Processed 11100 pages\n",
      "Processed 11200 pages\n",
      "Processed 11300 pages\n",
      "Processed 11400 pages\n",
      "Processed 11500 pages\n",
      "Processed 11600 pages\n",
      "Processed 11700 pages\n",
      "Processed 11800 pages\n",
      "Processed 11900 pages\n",
      "Processed 12000 pages\n",
      "Processed 12100 pages\n",
      "Processed 12200 pages\n",
      "Processed 12300 pages\n",
      "Processed 12400 pages\n",
      "Processed 12500 pages\n",
      "Processed 12600 pages\n",
      "Processed 12700 pages\n",
      "Processed 12800 pages\n",
      "Processed 12900 pages\n",
      "Processed 13000 pages\n",
      "Processed 13100 pages\n",
      "Processed 13200 pages\n",
      "Processed 13300 pages\n",
      "Processed 13400 pages\n",
      "Processed 13500 pages\n",
      "Processed 13600 pages\n",
      "Processed 13700 pages\n",
      "Processed 13800 pages\n",
      "Processed 13900 pages\n",
      "Processed 14000 pages\n",
      "Processed 14100 pages\n",
      "Processed 14200 pages\n",
      "Processed 14300 pages\n",
      "Processed 14400 pages\n",
      "Processed 14500 pages\n",
      "Processed 14600 pages\n",
      "Processed 14700 pages\n",
      "Processed 14800 pages\n",
      "Processed 14900 pages\n",
      "Processed 15000 pages\n",
      "Processed 15100 pages\n",
      "Processed 15200 pages\n",
      "Processed 15300 pages\n",
      "Processed 15400 pages\n",
      "Processed 15500 pages\n",
      "Processed 15600 pages\n",
      "Processed 15700 pages\n",
      "Processed 15800 pages\n",
      "Processed 15900 pages\n",
      "Processed 16000 pages\n",
      "Processed 16100 pages\n",
      "Processed 16200 pages\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "loaded_pd_songs = pd.read_csv('data/songs_meta_data.csv')\n",
    "for idx, (url, file_path) in enumerate(loaded_pd_songs.values):\n",
    "    abs_file_path = \"data/\"+file_path\n",
    "    if os.path.exists(abs_file_path):\n",
    "        continue\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n",
    "    with open(abs_file_path,'w+') as f:\n",
    "        for tag in soup.find_all('p'):\n",
    "            br_tags = tag.find_all('br')\n",
    "            for br_tag in br_tags:\n",
    "                br_tag.replace_with(\"\\n\")\n",
    "            content = tag.get_text() +'\\n'\n",
    "            if len(content) > 2:\n",
    "                f.write(content+'\\n')\n",
    "    if idx %100 == 0:\n",
    "        print('Processed {} pages'.format(idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 151 chuong : ta-xac-chet-phong-dang-khong-bi-troi-buoc\n",
      "Done 2264 chuong : thien-dao-do-thu-quan-convert\n",
      "Done 3171 chuong : tu-chan-lieu-thien-quan\n",
      "Done 2652 chuong : xuyen-nhanh-nam-than-co-chu%CC%81t-chay!\n",
      "Done 475 chuong : nu-phu-khong-lan-vao-khoai-xuyen\n",
      "Done 337 chuong : dap-noi-ban-sat-di-hoc\n",
      "Done 252 chuong : xuyen-thu-tho-he-kho-nu\n",
      "Done 1192 chuong : nhan-vat-phan-dien-hom-nay-cung-that-ngoan\n",
      "Done 1666 chuong : khoai-xuyen-he-thong-nhan-vat-phan-dien-boss-dot-kich!\n",
      "Done 758 chuong : tieu-yeu-the\n",
      "Done 704 chuong : su-phu-lai-rot-tuyen\n",
      "Done 2020 chuong : khoai-xuyen-moi-lan-deu-la-ta-nam-thuong\n",
      "Done 1144 chuong : khai-cuc-nu-ma-dau-phu-ta\n",
      "Done 932 chuong : cau-tai-yeu-vu-loan-the-tu-tien\n",
      "Done 4934 chuong : than-dao-dan-ton\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "link_truyen:str = 'https://truyen.tangthuvien.vn/doc-truyen/{ten_truyen}/chuong-{chuong}'\n",
    "so_chuong_truyen = {\n",
    "    \"ta-xac-chet-phong-dang-khong-bi-troi-buoc\" : 151,\n",
    "    \"thien-dao-do-thu-quan-convert\": 2264,\n",
    "    \"tu-chan-lieu-thien-quan\": 3171,\n",
    "    \"xuyen-nhanh-nam-than-co-chu%CC%81t-chay!\" : 2652,\n",
    "    \"nu-phu-khong-lan-vao-khoai-xuyen\" : 475,\n",
    "    \"dap-noi-ban-sat-di-hoc\" : 337,\n",
    "    \"xuyen-thu-tho-he-kho-nu\" : 252,\n",
    "    \"nhan-vat-phan-dien-hom-nay-cung-that-ngoan\" : 1192,\n",
    "    \"khoai-xuyen-he-thong-nhan-vat-phan-dien-boss-dot-kich!\" : 1666,\n",
    "    \"tieu-yeu-the\" : 758,\n",
    "    \"su-phu-lai-rot-tuyen\" : 704,\n",
    "    \"khoai-xuyen-moi-lan-deu-la-ta-nam-thuong\" : 2020,\n",
    "    \"khai-cuc-nu-ma-dau-phu-ta\": 1144,\n",
    "    \"cau-tai-yeu-vu-loan-the-tu-tien\": 932,\n",
    "    \"than-dao-dan-ton\": 4934,\n",
    "}\n",
    "\n",
    "base_data = 'data/truyen_from_tangthuvien_vn/'\n",
    "cols = ['ten_truyen','truyen_url','file_path','segment_file_path']\n",
    "truyen_meta_data = []\n",
    "for ten_truyen in so_chuong_truyen:\n",
    "    abs_folder = base_data+ten_truyen+'/'\n",
    "    os.makedirs(os.path.dirname(abs_folder), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(abs_folder+'raw/'), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(abs_folder+'segment/'), exist_ok=True)\n",
    "    for chuong in range(1,so_chuong_truyen[ten_truyen] + 1):\n",
    "        raw_file_path  = abs_folder+f\"raw/chuong_{chuong}\"+'.txt'\n",
    "        segment_file_paths  = abs_folder+f\"segment/chuong_{chuong}\"+'.txt'\n",
    "        link = link_truyen.format(ten_truyen=ten_truyen,chuong=chuong)\n",
    "        truyen_meta_data.append([ten_truyen,link,raw_file_path,segment_file_paths])\n",
    "\n",
    "        if os.path.exists(raw_file_path) :\n",
    "            continue\n",
    "        response = requests.get(link)\n",
    "        content = response.content\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        \n",
    "        tags = soup.find_all('div', class_=lambda x: x and x.startswith('box-chap'))\n",
    "        text= \"\"\n",
    "        for tag in tags:\n",
    "            text += tag.get_text() + \" \"\n",
    "        with open(raw_file_path,'w+') as f:\n",
    "            f.write(text)\n",
    "    print(f\"Done {chuong} chuong : {ten_truyen}\")\n",
    "\n",
    "pd_truyen = pd.DataFrame(truyen_meta_data,columns=cols)\n",
    "pd_truyen.to_csv('data/truyen_from_tangthuvien_vn/truyen_meta_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta-xac-chet-phong-dang-khong-bi-troi-buoc\n",
      "thien-dao-do-thu-quan-convert\n",
      "tu-chan-lieu-thien-quan\n",
      "xuyen-nhanh-nam-than-co-chu%CC%81t-chay!\n",
      "nu-phu-khong-lan-vao-khoai-xuyen\n",
      "dap-noi-ban-sat-di-hoc\n",
      "xuyen-thu-tho-he-kho-nu\n",
      "nhan-vat-phan-dien-hom-nay-cung-that-ngoan\n",
      "khoai-xuyen-he-thong-nhan-vat-phan-dien-boss-dot-kich!\n",
      "tieu-yeu-the\n",
      "su-phu-lai-rot-tuyen\n",
      "khoai-xuyen-moi-lan-deu-la-ta-nam-thuong\n"
     ]
    }
   ],
   "source": [
    "for i in so_chuong_truyen:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pd_songs = pd.read_csv('data/song_from_lyricvn_com/songs_meta_data.csv')\n",
    "for idx, (url, file_path) in enumerate(loaded_pd_songs.values):\n",
    "    abs_file_path = \"data/\"+file_path\n",
    "    content = open(abs_file_path,'r').read()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tên bài hát: Cứ Ngủ Say\n",
      " Ca sĩ: Khởi My, Chế Đình Cường\n",
      " Sáng tác: unknown\n",
      " Album: Sao Đom Đóm\n",
      " Ngày ra mắt: 01/01/1970\n",
      " Thể loại: Việt Nam, Nhạc Trẻ, V-Pop\n",
      "\n",
      "Bài Hát: Cứ́ Ngủ Say\n",
      " Ca Sĩ: Khởi My\n",
      "\n",
      "Lắng nghe\n",
      " từng giọt nắng lên đầy\n",
      " Lắng nghe\n",
      " mùa thu đến sớm nay\n",
      " Có lẽ lúc này\n",
      " anh yêu vẫn ngủ say\n",
      " Lòng em nhớ anh\n",
      " thật nhiều\n",
      "\n",
      "Lắng nghe\n",
      " từng góc phố ồn ào\n",
      " Hát lên\n",
      " lòng vui sướng biết bao\n",
      " Đến lúc thức dậy\n",
      " anh yêu em biết mấy\n",
      " Lòng anh nhớ em\n",
      " thật nhiều\n",
      "\n",
      "Dẫu biết có lúc giận hờn\n",
      " có lúc ngại ngần\n",
      " Vẫn thấy tim em\n",
      " gần anh hơn\n",
      " Nhớ lắm ánh mắt còn gầy\n",
      " tiếng hát còn đầy\n",
      " Vẫn thấy trong em\n",
      " cơn ngủ say\n",
      " Có lắm ước muốn tràn trề\n",
      " phút chốc ùa về\n",
      " phút chốc qua như\n",
      " là cơn mê\n",
      " Vẫn biết có lúc nhìn lại\n",
      " những tháng ngày dài\n",
      " Ánh mắt ban mai\n",
      " dành tặng anh\n",
      "\n",
      "Lắng nghe\n",
      " từng giọt nắng lên đầy\n",
      " Lắng nghe\n",
      " mùa thu đến sớm nay\n",
      " Có lẽ lúc này\n",
      " anh yêu vẫn ngủ say\n",
      " Lòng em nhớ anh\n",
      " thật nhiều\n",
      "\n",
      "Lắng nghe\n",
      " từng góc phố ồn ào\n",
      " Hát lên\n",
      " lòng vui sướng biết bao\n",
      " Đến lúc thức dậy\n",
      " anh yêu em biết mấy\n",
      " Lòng anh nhớ em\n",
      " thật nhiều\n",
      "\n",
      "Dẫu biết có lúc giận hờn\n",
      " có lúc ngại ngần\n",
      " Vẫn thấy tim em\n",
      " gần anh hơn\n",
      " Nhớ lắm ánh mắt còn gầy\n",
      " tiếng hát còn đầy\n",
      " Vẫn thấy trong em\n",
      " cơn ngủ say\n",
      " Có lắm ước muốn tràn trề\n",
      " phút chốc ùa về\n",
      " phút chốc qua như\n",
      " là cơn mê\n",
      " Vẫn biết có lúc nhìn lại\n",
      " những tháng ngày dài\n",
      " Ánh mắt ban mai\n",
      " dành tặng anh\n",
      " Dẫu biết có lúc giận hờn\n",
      " có lúc ngại ngần\n",
      " Vẫn thấy tim em\n",
      " gần anh hơn\n",
      " Nhớ lắm ánh mắt còn gầy\n",
      " tiếng hát còn đầy\n",
      " Vẫn thấy trong em\n",
      " cơn ngủ say\n",
      " Có lắm ước muốn tràn trề\n",
      " phút chốc ùa về\n",
      " phút chốc qua như\n",
      " là cơn mê\n",
      " Nhớ lắm ánh mắt còn gầy\n",
      " tiếng hát còn đầy\n",
      " Vẫn thấy trong em\n",
      " cơn ngủ say\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V\n",
      "i\n",
      "ệ\n",
      "t\n",
      "N\n",
      "a\n",
      "m\n",
      "ỳ\n",
      "ử\n",
      "k\n",
      "ì\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "vietnamese_letters_pattern = re.compile(r'[A-Za-zÀ-ỹ]')\n",
    "\n",
    "text = \"Việt Nam ỳ ử kì\"\n",
    "matches = vietnamese_letters_pattern.findall(text)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
